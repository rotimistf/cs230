{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl  \n",
    "from tensorflow.python.framework import ops \n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c4a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce16b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize an array\n",
    "def fn_normalize_col(x):\n",
    "    if x.dtype == 'object' :\n",
    "        return x\n",
    "    return (x - np.mean(x))/np.std(x)\n",
    "\n",
    "#Convert array to tensor and specify shape\n",
    "def fn_target_to_tensor(data) :\n",
    "    return tf.reshape(tf.convert_to_tensor(data),[data.shape[0],1])\n",
    " \n",
    "#Get and normalize data\n",
    "def fn_get_fusion_coral_data(cols_X1, cols_X2, cols_X3, cols_Y, cols_norm, train_prob = 0.9, seed = 0) : \n",
    "    \n",
    "    data = pd.read_csv(\n",
    "    \"data//coral_total_cover.csv\",header=0).dropna()\n",
    "    \n",
    "    X1 = data.copy().filter(cols_X1)\n",
    "    X2 = data.copy().filter(cols_X2)\n",
    "    X3 = data.copy().filter(cols_X3)\n",
    "    \n",
    "    Y = data.copy().filter(cols_Y)\n",
    "   \n",
    "    Y = (Y > 8) * 1 \n",
    "    X1[[\"o_ocean\"]] = (X1[[\"o_ocean\"]] == \"CARIB\" ) * 1\n",
    "    \n",
    "    X1 = X1.apply(fn_normalize_col,axis=0)\n",
    "    X2 = X2.apply(fn_normalize_col,axis=0)\n",
    "    X3 = X3.apply(fn_normalize_col,axis=0)\n",
    "    \n",
    "    return fn_fusion_split_data(X1,X2,X3, Y, train_prob, seed) \n",
    "\n",
    "#Split data into training and test sets using a defined probability\n",
    "def fn_fusion_split_data(X1,X2,X3,Y, train_prob = 0.9, seed = 0) : \n",
    "    \n",
    "    X1_train = X1.sample(frac=train_prob, random_state=seed) \n",
    "    X1_test   = X1.drop(X1_train.index)\n",
    "    X2_train = X2.loc[X1_train.index] \n",
    "    X2_test = X2.drop(X1_train.index) \n",
    "    X3_train = X3.loc[X1_train.index] \n",
    "    X3_test = X3.drop(X1_train.index) \n",
    "    \n",
    "    Y_train = Y.loc[X1_train.index] \n",
    "    Y_test = Y.drop(X1_train.index) \n",
    "    \n",
    "    X1_train=tf.convert_to_tensor(X1_train) \n",
    "    X1_test=tf.convert_to_tensor(X1_test)\n",
    "    X2_train=tf.convert_to_tensor(X2_train) \n",
    "    X2_test=tf.convert_to_tensor(X2_test)\n",
    "    X3_train=tf.convert_to_tensor(X3_train) \n",
    "    X3_test=tf.convert_to_tensor(X3_test)\n",
    "\n",
    "    Y_train=fn_target_to_tensor(Y_train) \n",
    "    Y_test=fn_target_to_tensor(Y_test) \n",
    "    \n",
    "    return [X1_train,X1_test,X2_train,X2_test,X3_train,X3_test,Y_train,Y_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b528c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hyperband tunable model\n",
    "def tunable_model(hp) :\n",
    "    \n",
    "    hp_g1_units = hp.Int('units_g1', min_value=40, max_value=256, step=32)\n",
    "    hp_g2_units = hp.Int('units_g2', min_value=40, max_value=256, step=32)\n",
    "    hp_e1_units = hp.Int('units_e1', min_value=40, max_value=256, step=32)\n",
    "    hp_e2_units = hp.Int('units_e2', min_value=40, max_value=256, step=32)\n",
    "    hp_h1_units = hp.Int('units_h1', min_value=40, max_value=256, step=32)\n",
    "    hp_h2_units = hp.Int('units_h2', min_value=40, max_value=256, step=32)\n",
    "    hp_1_units = hp.Int('units_1', min_value=40, max_value=128, step=32)\n",
    "    hp_2_units = hp.Int('units_2', min_value=40, max_value=128, step=32)\n",
    "    hp_l_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    geo_input = tf.keras.Input(shape=(3,)) \n",
    "    geo_out = tfl.Dense(hp_g1_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(geo_input) \n",
    "    geo_out = tfl.Dense(hp_g2_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(geo_out) \n",
    "    geo_out = tf.keras.layers.Dropout(rate=0.5)(geo_out) \n",
    "    geo_out = tfl.Dense(1, activation='sigmoid')(geo_out)\n",
    "\n",
    "    env_input = tf.keras.Input(shape=(6,)) \n",
    "    env_out = tfl.Dense(hp_e1_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(env_input) \n",
    "    env_out = tfl.Dense(hp_e2_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(env_out) \n",
    "    env_out = tf.keras.layers.Dropout(rate=0.5)(env_out) \n",
    "    env_out = tfl.Dense(1, activation='sigmoid')(env_out)\n",
    "\n",
    "    human_input = tf.keras.Input(shape=(2,)) \n",
    "    human_out = tfl.Dense(hp_h1_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(human_input) \n",
    "    human_out = tfl.Dense(hp_h2_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(human_out) \n",
    "    human_out = tfl.Dense(hp_2_units, activation='sigmoid')(human_out)\n",
    "\n",
    "    X_late_fusion = tfl.concatenate([geo_out, env_out, human_out]) \n",
    "    X_late_fusion = tfl.Dense(hp_1_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(X_late_fusion) \n",
    "    X_late_fusion = tfl.Dense(hp_2_units, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(X_late_fusion)\n",
    "    late_fusion_out = tfl.Dense(1, activation='sigmoid')(X_late_fusion)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[geo_input,env_input,human_input], outputs=late_fusion_out, name=\"deepcoral_late_fusion\")\n",
    " \n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_l_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),  tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae398c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get training and test data\n",
    "col_Y =  [\"o_coral_cover\"]\n",
    "cols_X = [\"o_ocean\", \"o_lat\", \"o_long\", \"o_depth\",\n",
    "           \"o_human_pop50\", \"o_land_area50\", \"o_storm_30yr\",\"o_l_att_mean\", \"o_npp_mean\", \"o_sst_mean\", \"o_wave_mean\"]\n",
    "\n",
    "\n",
    "cols_geo = [\"o_ocean\",\"o_lat\", \"o_long\"]\n",
    "\n",
    "\n",
    "cols_env = [\"o_depth\", \"o_storm_30yr\",\"o_l_att_mean\", \"o_npp_mean\", \"o_sst_mean\", \"o_wave_mean\"]\n",
    "\n",
    "\n",
    "cols_human = [\"o_human_pop50\", \"o_land_area50\"]\n",
    "\n",
    "    \n",
    "cols_norm  = [\"o_lat\", \"o_long\", \"o_depth\",\n",
    "           \"o_human_pop50\", \"o_land_area50\", \"o_storm_30yr\",\"o_l_att_mean\", \"o_npp_mean\", \"o_sst_mean\", \"o_wave_mean\"] \n",
    "\n",
    "X_geo_train,X_geo_test,X_env_train,X_env_test,X_human_train,X_human_test,Y_train,Y_test = fn_get_fusion_coral_data(cols_geo,cols_env,cols_human,col_Y,cols_norm,train_prob=0.9, seed=230) \n",
    "X_train = [X_geo_train,X_env_train,X_human_train]\n",
    "X_test = [X_geo_test,X_env_test,X_human_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89ce77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure Tuner\n",
    "\n",
    "tuner = kt.Hyperband(tunable_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=1000,\n",
    "                     project_name='deepreef_XL',\n",
    "                     factor=3)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3501db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "tuner.search([X_geo_train,X_env_train,X_human_train], Y_train, epochs=300, validation_split=0.1, callbacks=[stop_early])\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "\n",
    "print(f\"\"\"optimal number of units are \n",
    "{best_hps.get('units_g1')}, {best_hps.get('units_g2')}, \n",
    "{best_hps.get('units_e1')},{best_hps.get('units_e2')}, \n",
    "{best_hps.get('units_h1')}, {best_hps.get('units_h2')},\n",
    "{best_hps.get('units_1')},and {best_hps.get('units_2')} and the optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0810ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Optimal Epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, Y_train, epochs=300, validation_split=0.1)\n",
    "\n",
    "val_prec_per_epoch = np.array(history.history['val_precision'])\n",
    "val_recall_per_epoch = np.array(history.history['val_recall'])\n",
    "\n",
    "val_f1_per_epoch = (2 * ((val_prec_per_epoch * val_recall_per_epoch)/(val_prec_per_epoch + val_recall_per_epoch))).tolist()\n",
    "best_epoch = val_f1_per_epoch.index(max(val_f1_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d203f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get hyperband tuned model\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model until best epoch\n",
    "hypermodel.fit(X_train, Y_train, epochs=best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Test Set\n",
    "\n",
    "eval_result = hypermodel.evaluate(X_test, Y_test)\n",
    "print(\"[test loss, test accuracy, test f1]:\", eval_result[0],  eval_result[1], 2*((eval_result[2]*eval_result[3])/(eval_result[2]+eval_result[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute absolute mean of geography input weights\n",
    "abs(np.apply_along_axis(np.mean, 1,hypermodel.layers[2].get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute absolute mean of environment input weights\n",
    "abs(np.apply_along_axis(np.mean, 1,hypermodel.layers[3].get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute absolute mean of human input weights\n",
    "abs(np.apply_along_axis(np.mean, 1,hypermodel.layers[7].get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f602a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
